# -*- coding: utf-8 -*-
"""SCT_DS_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_OWDSICvGQWT1CDaZ3WUrl-Vs9fBjS2J
"""

# Import libraries for data analysis and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Import libraries for machine learning
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Set plot style
plt.style.use('seaborn-v0_8-whitegrid')
print("✅ Libraries imported successfully!")

"""***Data Exploration & Visualization***"""

# Load the training and testing data
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

print("--- Training Data Info ---")
train_df.info()
print("\n--- First 5 Rows of Training Data ---")
print(train_df.head())

# Let's start visualizing the most important relationships

# How many survived vs. died? (0 = No, 1 = Yes)
plt.figure(figsize=(6, 5))
sns.countplot(x='Survived', data=train_df)
plt.title('Survival Distribution (0 = Died, 1 = Survived)')
plt.show()

# Survival by Gender
plt.figure(figsize=(6, 5))
sns.countplot(x='Survived', hue='Sex', data=train_df, palette='viridis')
plt.title('Survival by Gender')
plt.show()

# Survival by Passenger Class
plt.figure(figsize=(6, 5))
sns.countplot(x='Survived', hue='Pclass', data=train_df, palette='plasma')
plt.title('Survival by Passenger Class')
plt.show()

# Age Distribution
plt.figure(figsize=(10, 6))
sns.histplot(data=train_df, x='Age', hue='Survived', kde=True, multiple='stack')
plt.title('Age Distribution by Survival')
plt.show()

"""***Data Cleaning***"""

# Save PassengerId for the submission file later
test_passenger_ids = test_df['PassengerId']

# Combine train and test data for consistent cleaning
# We drop 'Survived' from the training set for now
train_labels = train_df['Survived']
combined_df = pd.concat([train_df.drop('Survived', axis=1), test_df], axis=0)

print(f"Combined DataFrame shape: {combined_df.shape}")
print("\n--- Missing Values Before Cleaning ---")
print(combined_df.isnull().sum())

# --- Handle Missing Values ---

# Age: Fill missing ages with the median age of their passenger class
median_age = combined_df.groupby('Pclass')['Age'].transform('median')
combined_df['Age'].fillna(median_age, inplace=True)

# Embarked: Fill the two missing values with the most common port (mode)
mode_embarked = combined_df['Embarked'].mode()[0]
combined_df['Embarked'].fillna(mode_embarked, inplace=True)

# Fare: Fill the one missing fare in the test set with the median
median_fare = combined_df['Fare'].median()
combined_df['Fare'].fillna(median_fare, inplace=True)

# Cabin: This column has too many missing values, so we'll drop it for this model
combined_df.drop('Cabin', axis=1, inplace=True)

print("\n--- Missing Values After Cleaning ---")
print(combined_df.isnull().sum())

"""***Feature Engineering***"""

# --- Convert Categorical Features to Numbers ---

# Sex: Convert 'male' to 0 and 'female' to 1
combined_df['Sex'] = combined_df['Sex'].map({'male': 0, 'female': 1})

# Embarked: Use one-hot encoding to create numerical columns for each port
embarked_dummies = pd.get_dummies(combined_df['Embarked'], prefix='Embarked')
combined_df = pd.concat([combined_df, embarked_dummies], axis=1)

# --- Create New Features ---

# FamilySize: Combine SibSp (siblings/spouses) and Parch (parents/children)
combined_df['FamilySize'] = combined_df['SibSp'] + combined_df['Parch'] + 1

# IsAlone: A simple feature to check if the passenger was alone
combined_df['IsAlone'] = (combined_df['FamilySize'] == 1).astype(int)

# --- Final Cleanup ---
# Drop columns that are no longer needed for the model
columns_to_drop = ['Name', 'Ticket', 'Embarked', 'PassengerId']
combined_df.drop(columns_to_drop, axis=1, inplace=True)

print("\n--- Data After Feature Engineering ---")
print(combined_df.head())

"""***Model Building***"""

# Split the data back into training and testing sets
train_len = len(train_df)
X_train_final = combined_df[:train_len]
X_test_final = combined_df[train_len:]
y_train_final = train_labels

print(f"Training features shape: {X_train_final.shape}")
print(f"Test features shape: {X_test_final.shape}")

# Split the training data for validation
X_train, X_val, y_train, y_val = train_test_split(
    X_train_final, y_train_final, test_size=0.2, random_state=42
)

# Initialize and train the RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
model.fit(X_train, y_train)

print("\n✅ Model trained successfully!")

"""***Model Evaluation***"""

# Make predictions on the validation set
y_pred = model.predict(X_val)

# Calculate accuracy
accuracy = accuracy_score(y_val, y_pred)
print(f"Validation Accuracy: {accuracy:.4f} ({accuracy:.2%})")

# Print a detailed classification report
print("\n--- Classification Report ---")
print(classification_report(y_val, y_pred))

# Show the confusion matrix
print("\n--- Confusion Matrix ---")
cm = confusion_matrix(y_val, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.show()

"""***Submission***"""

# Make predictions on the final test data
test_predictions = model.predict(X_test_final)

# Create the submission DataFrame
submission_df = pd.DataFrame({
    'PassengerId': test_passenger_ids,
    'Survived': test_predictions
})

# Save the submission file
submission_df.to_csv('titanic_submission.csv', index=False)

print("✅ Submission file 'titanic_submission.csv' created successfully!")
print("\n--- First 5 rows of submission file ---")
print(submission_df.head())

